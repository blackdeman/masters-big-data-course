--- ЧАСТЬ 1 ---
Для создания и заполнения таблиц в hbase использовалась техника bulkload.
Процесс состоит из нескольких шагов (скрипт hbase-demchenko-ext.sh в папке task3-hbase/hbase-bulk-import):
1. Копирую файл с описанием графа во временную папку в hdfs;
2. Создаю соответствующей таблицы в hbase;
3. С помощью кастомного map reduce job'а (проект находится в папке task3-hbase/hbase-bulk-import) создаю hfile для импорта в hbase;
4. Загружаю hfile в таблицу с помощью стандартного модуля LoadIncrementalHFiles.

В самом конце выводится результат операции count и удаляются временные файлы.

Данные хранятся в таблице следующим образом:
	- таблица имеет всего одну column family с именем cf;
	- в качестве RowKey я использую номер вершины, в которую входит ребро;
	- в качестве column qualifier номер вершины, из который ребро выходит;
	- в качестве значения - вес ребра.

Итого информация о ребре (a, b) с весом w у меня находится в строке с RowKey = b, в столбце cf:a, где значением является w.

--- ЧАСТЬ 2 ---
Измененное решение С1021 находится в папке task3-hbase/spark-with-hbase, там же находятся результаты работы программы.
Сравнить решения можно скриптом task3-hbase/compare-results.sh, при условии что структура папок совпадает с моей (результаты совпадают).

Измерение времени проводилось такой строчкой в консоли:

	START=$(date +%s.%N); ./run?.sh; END=$(date +%s.%N); echo $(echo "$END - $START" | bc);

где вместо run?.sh подставлялись скрипты из второго и третьего заданий

Результаты получились следующими:

task2:
	graph1 - 11.6 s
	graph2 - 68.1 s

task3:
	graph1 - 14.2 s
	graph2 - 71.5 s
	
Видно, что версия с загрузкой данных из hbase немного проигрывает, хотя порядок значений приблизительно одинаковый.
Это можно объяснить накладными расходами на использование базы данных, ведь в обоих случаях по сути происходит последовательное чтение всех записей.
Хотя, не исключено что более гибкая настройка hbase и оптимизация программы на spark могла бы дать другие результаты.